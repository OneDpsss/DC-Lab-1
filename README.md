# DC-Lab-1

## Задание 1.
На каждом из графиков представдено время выполнения, ускорение и эффективность алгоритма на различных размерах матриц при разбиении по строкам, по столбцам и по блокам для количества потоков P = 1, 2, 4, 16.

### Графики параллельного выполнения:

<img width="761" height="458" alt="image" src="https://github.com/user-attachments/assets/a45a926a-70b0-494c-847c-f0cf231c82f3" />

### Графики последовательного выполнения:
<img width="761" height="458" alt="image" src="https://github.com/user-attachments/assets/19010d17-937a-4643-a713-153bfdca195a" />

### Графики по ускорению:

<img width="761" height="458" alt="image" src="https://github.com/user-attachments/assets/9fb9a9d7-fe95-41ad-959a-6042c35191a4" />

### Графики по эффективности:

<img width="761" height="458" alt="image" src="https://github.com/user-attachments/assets/aa144648-06d6-4134-8c76-7f64d031b01b" />

### Выводы по заданию 1.

Анализ результатов показывает, что параллельная версия демонстрирует ускорение только при достаточном объеме вычислений. Для малых задач (n ≤ 6) параллелизация неэффективна из-за доминирования накладных расходов на коммуникации над полезными вычислениями. При увеличении объема данных (n ≥ 9) наблюдается устойчивый рост ускорения, особенно для P=4, что подтверждает масштабируемость алгоритма на больших задачах.

Максимальная эффективность достигается при P=4 и n=12-14, демонстрируя явление сверхлинейного ускорения благодаря оптимальному использованию кэш-памяти. Однако при P=16 эффективность резко падает до 1-2%, что объясняется перегрузкой коммуникационной подсистемы и малым объемом вычислений на процесс. Ускорение при P=16 не превышает 16x при теоретическом максимуме 16, что указывает на значительные накладные расходы.

В целом, алгоритм эффективен только при достаточно больших n и умеренном числе процессов (P=2-4), где достигается баланс между вычислительной нагрузкой и коммуникационными затратами.


## Задание 2.
На каждом из графиков представдено время выполнения, ускорение и эффективность алгоритма на различных размерах матриц при разбиении по строкам, по столбцам и по блокам для количества потоков P = 1, 4, 16.

### Графики параллельного выполнения:

<img width="761" height="458" alt="image" src="https://github.com/user-attachments/assets/146eb8de-24d4-4641-b1e6-c1dc1f869ee9" />

<img width="761" height="455" alt="image" src="https://github.com/user-attachments/assets/4492d6a9-3107-417a-a783-0785896dee83" />

<img width="762" height="458" alt="image" src="https://github.com/user-attachments/assets/c0231e47-7fa9-48f1-9b4a-73aa3ff1b03f" />

### Графики последовательного выполнения:

<img width="761" height="454" alt="image" src="https://github.com/user-attachments/assets/bceda8a5-4219-45bf-b837-145c6c03640d" />

<img width="763" height="452" alt="image" src="https://github.com/user-attachments/assets/b1ed1b96-dace-43e4-9467-06693f3be683" />

<img width="758" height="456" alt="image" src="https://github.com/user-attachments/assets/6d2200b5-33fb-4f54-b7d6-bfc8f5b05a96" />



### Графики по ускорению:

<img width="694" height="410" alt="image" src="https://github.com/user-attachments/assets/0ba06896-5924-444b-9c0b-f03be973ef13" />

<img width="697" height="412" alt="image" src="https://github.com/user-attachments/assets/62924570-b03f-4f9f-89b8-af4c4fdbc550" />

<img width="698" height="415" alt="image" src="https://github.com/user-attachments/assets/17e33ba6-4c46-4c4a-92da-2da7f17511cd" />

### Графики по эффективности:

<img width="696" height="410" alt="image" src="https://github.com/user-attachments/assets/3c2d3c2f-ae45-495c-b2fc-ceb496f0f7aa" />

<img width="689" height="413" alt="image" src="https://github.com/user-attachments/assets/6b138476-369f-48e3-8a61-63e98779e058" />

<img width="696" height="414" alt="image" src="https://github.com/user-attachments/assets/d43c8312-fa3a-4912-8a83-8b2e75e8af55" />

### Выводы по заданию 2.
При малых размерах (N <= 64) параллелизация неэффективна: накладные расходы на коммуникацию превышают выигрыш от распределения вычислений. Это происходит потому, что объем вычислений мал, а коммуникационные затраты остаются постоянными.
При больших размерах (N >= 512) параллелизация становится эффективной, так как вычислительная сложность растет квадратично (O(N²)), а коммуникационные затраты растут линейно. Время вычислений начинает доминировать над временем коммуникации.


Падение эффективности с ростом P объясняется законом Амдала и ростом коммуникационных затрат. При увеличении количества процессов растет число сообщений между процессами, а последовательная часть программы (инициализация, сбор результатов) ограничивает ускорение.
Улучшение производительности с ростом N происходит потому, что вычислительная работа растет быстрее коммуникационной. При больших матрицах время вычислений доминирует, делая параллелизацию выгодной.


## Задание 3.
На каждом из графиков представлено время при параллельном и последовательном умножении, а также ускорение и эффективность для P = 1, 4, 16

<img width="1039" height="665" alt="image" src="https://github.com/user-attachments/assets/23e2aa21-bf16-435d-ad3a-189c3c7e450d" />

<img width="1039" height="665" alt="image" src="https://github.com/user-attachments/assets/25408108-6c97-43d0-bd6f-356de94c6210" />

<img width="1055" height="674" alt="image" src="https://github.com/user-attachments/assets/31cb8c9b-8d1f-4eca-b66d-ea7b1a9fa6cb" />

<img width="1136" height="664" alt="image" src="https://github.com/user-attachments/assets/8aa77910-c3a9-406c-aaf5-04af2a1fddf2" />

### Выводы
Анализ результатов показывает, что алгоритм Кэннона демонстрирует ускорение при увеличении числа процессов, однако эффективность снижается с ростом P. Для малых матриц (N ≤ 64) параллельное ускорение незначительно или отсутствует из-за доминирования накладных расходов на коммуникации. При увеличении размера матрицы (N ≥ 256) наблюдается устойчивый рост ускорения, особенно для P=4 и P=16, что указывает на масштабируемость алгоритма на больших задачах. 

Максимальная эффективность достигается при N=64 и P=4 (около 22%), а при N=1024 и P=16 — около 14%, что подтверждает необходимость баланса между вычислительной нагрузкой и коммуникационными затратами. Ускорение при P=16 не превышает 2.5, что объясняется высокой долей обмена данными в общем времени выполнения. В целом, алгоритм эффективен только при достаточно больших N и умеренном числе процессов (P=4), а при P=16 его эффективность падает из-за перегрузки сетевой подсистемы.
